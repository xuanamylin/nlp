{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_3L3L50eDORecDO.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZSvNkDKwZR4"
      },
      "source": [
        "- 3-layer LSTM encoder + 3-layer LSTM decoder\n",
        "- 50 epoches\n",
        "- Dropout + Recurrent dropout\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLGprNm5TXM3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "7584fa23-548b-4867-8c29-ce746edd2a8d"
      },
      "source": [
        "!pip install nltk==3.5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nltk==3.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\n",
            "\r\u001b[K     |▎                               | 10kB 17.0MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 22.4MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 25.0MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 27.5MB/s eta 0:00:01\r\u001b[K     |█▏                              | 51kB 28.9MB/s eta 0:00:01\r\u001b[K     |█▍                              | 61kB 30.8MB/s eta 0:00:01\r\u001b[K     |█▋                              | 71kB 32.4MB/s eta 0:00:01\r\u001b[K     |█▉                              | 81kB 26.6MB/s eta 0:00:01\r\u001b[K     |██                              | 92kB 28.0MB/s eta 0:00:01\r\u001b[K     |██▎                             | 102kB 29.4MB/s eta 0:00:01\r\u001b[K     |██▌                             | 112kB 29.4MB/s eta 0:00:01\r\u001b[K     |██▊                             | 122kB 29.4MB/s eta 0:00:01\r\u001b[K     |███                             | 133kB 29.4MB/s eta 0:00:01\r\u001b[K     |███▏                            | 143kB 29.4MB/s eta 0:00:01\r\u001b[K     |███▍                            | 153kB 29.4MB/s eta 0:00:01\r\u001b[K     |███▋                            | 163kB 29.4MB/s eta 0:00:01\r\u001b[K     |███▉                            | 174kB 29.4MB/s eta 0:00:01\r\u001b[K     |████▏                           | 184kB 29.4MB/s eta 0:00:01\r\u001b[K     |████▍                           | 194kB 29.4MB/s eta 0:00:01\r\u001b[K     |████▋                           | 204kB 29.4MB/s eta 0:00:01\r\u001b[K     |████▉                           | 215kB 29.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 225kB 29.4MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 235kB 29.4MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 245kB 29.4MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 256kB 29.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 266kB 29.4MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 276kB 29.4MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 286kB 29.4MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 296kB 29.4MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 307kB 29.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 317kB 29.4MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 327kB 29.4MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 337kB 29.4MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 348kB 29.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 358kB 29.4MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 368kB 29.4MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 378kB 29.4MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 389kB 29.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 399kB 29.4MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 409kB 29.4MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 419kB 29.4MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 430kB 29.4MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 440kB 29.4MB/s eta 0:00:01\r\u001b[K     |██████████                      | 450kB 29.4MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 460kB 29.4MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 471kB 29.4MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 481kB 29.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 491kB 29.4MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 501kB 29.4MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 512kB 29.4MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 522kB 29.4MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 532kB 29.4MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 542kB 29.4MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 552kB 29.4MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 563kB 29.4MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 573kB 29.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 583kB 29.4MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 593kB 29.4MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 604kB 29.4MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 614kB 29.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 624kB 29.4MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 634kB 29.4MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 645kB 29.4MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 655kB 29.4MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 665kB 29.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 675kB 29.4MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 686kB 29.4MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 696kB 29.4MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 706kB 29.4MB/s eta 0:00:01\r\u001b[K     |████████████████                | 716kB 29.4MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 727kB 29.4MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 737kB 29.4MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 747kB 29.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 757kB 29.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 768kB 29.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 778kB 29.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 788kB 29.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 798kB 29.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 808kB 29.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 819kB 29.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 829kB 29.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 839kB 29.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 849kB 29.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 860kB 29.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 870kB 29.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 880kB 29.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 890kB 29.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 901kB 29.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 911kB 29.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 921kB 29.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 931kB 29.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 942kB 29.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 952kB 29.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 962kB 29.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 972kB 29.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 983kB 29.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 993kB 29.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.0MB 29.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.0MB 29.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.0MB 29.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.0MB 29.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.0MB 29.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.1MB 29.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.1MB 29.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1MB 29.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.1MB 29.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.1MB 29.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.1MB 29.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.1MB 29.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.1MB 29.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.1MB 29.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.1MB 29.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.2MB 29.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2MB 29.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.2MB 29.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.2MB 29.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.2MB 29.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.2MB 29.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.2MB 29.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.2MB 29.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.2MB 29.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.2MB 29.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.3MB 29.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.3MB 29.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.3MB 29.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.3MB 29.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.3MB 29.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.3MB 29.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.3MB 29.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.3MB 29.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.3MB 29.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.4MB 29.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.4MB 29.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.4MB 29.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.4MB 29.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.4MB 29.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.4MB 29.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.4MB 29.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.4MB 29.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.4MB 29.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from nltk==3.5) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from nltk==3.5) (0.15.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from nltk==3.5) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from nltk==3.5) (4.41.1)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.5-cp36-none-any.whl size=1434674 sha256=5d3661f859e800cefaf04f86d3e536fd4123a5a8fafb2a761c53d76de74e980c\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCwuKEeX2Re6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8b237747-049e-4b01-de94-7731b7425718"
      },
      "source": [
        "# Test Summarization with LSTM\n",
        "# Author: Xuan Lin\n",
        "\n",
        "# Save output to a file\n",
        "import sys\n",
        "import datetime as dt\n",
        "\n",
        "master_ts = dt.datetime.now() - dt.timedelta(hours = 5)\n",
        "ts_filename = format(master_ts, \"%m%d%H%M\")\n",
        "\n",
        "#full_path = '/content/drive/My Drive/Deep Learning Training/'\n",
        "#orig_stdout = sys.stdout\n",
        "#f = open(full_path + 'output_'+ts_filename+'.txt', 'w')\n",
        "#sys.stdout = f\n",
        "\n",
        "print(\"Deep Learning Group Project - Text Summarization\")\n",
        "print(\"Author: Xuan Lin\")\n",
        "print(\"\")\n",
        "print(\"File timestamp: {}\".format(ts_filename))\n",
        "print(\"\")\n",
        "\n",
        "# ------------------- Hyper-Parameters --------------------\n",
        "\n",
        "weights_to_load = ''\n",
        "print(\"Load model weights from {}\".format(weights_to_load))\n",
        "\n",
        "params = dict({'title': '3L50eDORecDO',\n",
        "               'n_epoch':50,\n",
        "              'batch_size': 500,\n",
        "              'optimizer': 'adam',\n",
        "              'loss_fun': 'sparse_categorical_crossentropy', # categorical_crossentropy\n",
        "              'max_len_text': 80,\n",
        "              'max_len_summary': 8,\n",
        "              'patience': 3,\n",
        "              'dropout': 0.1,\n",
        "              'rec_dropout': 0.1\n",
        "              })\n",
        "print(\"Hyper-parameters: {}\".format(params))\n",
        "print(\"\")\n",
        "\n",
        "# ---------------------- Load Packages -------------------------\n",
        "\n",
        "#import pkg_resources\n",
        "#pkg_resources.require(\"nltk==3.5\")\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from gensim import models\n",
        "from keras.preprocessing.text import Tokenizer \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import plot_model\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "#nltk.download('stopwords')\n",
        "from Attention import AttentionLayer\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(2020)\n",
        "\n",
        "print(\"Packages Loaded.\")\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "# --------------------- Load data ------------------------\n",
        "\n",
        "#path = '/content/drive/My Drive/'\n",
        "#df = pd.read_csv(path + \"Copy of Reviews Cleaned.csv\")\n",
        "full_path = '/content/drive/My Drive/Deep Learning Training/'\n",
        "\n",
        "#print(\"Dataset Size: {}\".format(df.shape))\n",
        "#print(\"\")\n",
        "\n",
        "max_len_text = params['max_len_text']\n",
        "max_len_summary = params['max_len_summary']\n",
        "\n",
        "# Load train, val, and test data from csv files\n",
        "train = pd.read_csv('/content/drive/My Drive/Deep Learning Training/Data Sets/TrainSet.csv')\n",
        "val = pd.read_csv('/content/drive/My Drive/Deep Learning Training/Data Sets/ValSet.csv')\n",
        "test= pd.read_csv('/content/drive/My Drive/Deep Learning Training/Data Sets/TestSet80k.csv')\n",
        "\n",
        "train_id, Xtrain, ytrain = train['id'], train['text'].astype('str'), train['summary'].astype('str')\n",
        "val_id, Xval, yval = val['id'], val['text'].astype('str'), val['summary'].astype('str')\n",
        "test_id, Xtest, ytest = test['id'], test['text'].astype('str'), test['summary'].astype('str')\n",
        "y_true = ytest\n",
        "\n",
        "\"\"\"\n",
        "# Train Test Split\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(df[['Id','cleaned_text']].astype('str'),\n",
        "                                              df['cleaned_summary'].astype('str'),\n",
        "                                              test_size=0.2,random_state = 2020)\n",
        "Xtrain, Xval, ytrain, yval = train_test_split(Xtrain, ytrain, test_size = 0.1, random_state = 2020)\n",
        "train_id, Xtrain = Xtrain['Id'], Xtrain['cleaned_text']\n",
        "val_id, Xval = Xval['Id'], Xval['cleaned_text']\n",
        "test_id, Xtest = Xtest['Id'], Xtest['cleaned_text']\n",
        "y_true = ytest\n",
        "\"\"\"\n",
        "\n",
        "print(\"Train X & y: \", Xtrain.shape, ytrain.shape)\n",
        "print(\"Validation X & y: \", Xval.shape, yval.shape)\n",
        "print(\"Test X & y: \", Xtest.shape, ytest.shape)\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "# ---------------------- Tokenization ----------------------\n",
        "\n",
        "# Review\n",
        "x_tokenizer = Tokenizer()\n",
        "x_tokenizer.fit_on_texts(list(Xtrain))\n",
        "Xtrain = x_tokenizer.texts_to_sequences(Xtrain) \n",
        "Xval = x_tokenizer.texts_to_sequences(Xval)\n",
        "Xtest = x_tokenizer.texts_to_sequences(Xtest)\n",
        "\n",
        "Xtrain = pad_sequences(Xtrain, maxlen=max_len_text, padding='post')\n",
        "Xval = pad_sequences(Xval, maxlen=max_len_text, padding='post')\n",
        "Xtest = pad_sequences(Xtest, maxlen=max_len_text, padding='post')\n",
        "x_voc_size = len(x_tokenizer.word_index) + 1\n",
        "\n",
        "print(\"Review tokenization and padding done.\")\n",
        "\n",
        "\n",
        "# Summary\n",
        "y_tokenizer = Tokenizer()\n",
        "y_tokenizer.fit_on_texts(list(ytrain))\n",
        "\n",
        "ytrain = y_tokenizer.texts_to_sequences(ytrain)\n",
        "yval = y_tokenizer.texts_to_sequences(yval)\n",
        "ytest = y_tokenizer.texts_to_sequences(ytest) \n",
        "\n",
        "ytrain = pad_sequences(ytrain, maxlen=max_len_summary, padding='post')\n",
        "yval = pad_sequences(yval, maxlen=max_len_summary, padding='post')\n",
        "ytest = pad_sequences(ytest, maxlen=max_len_summary, padding='post')\n",
        "y_voc_size = len(y_tokenizer.word_index) + 1\n",
        "\n",
        "print(\"Summary tokenization and padding done.\")\n",
        "print(\"\")\n",
        "\n",
        "#print(\"Sample Review: {}\".format(Xtrain[0]))\n",
        "\n",
        "\n",
        "\n",
        "# ------------------ LSTM Training Model -------------------\n",
        "\n",
        "\n",
        "from keras import backend as K\n",
        "K.clear_session()\n",
        "\n",
        "latent_dim = 500   # dimension of embedding\n",
        "\n",
        "# Encoder ----------\n",
        "\n",
        "# Inputs are review entries, which has 80 words max\n",
        "encoder_input = Input(shape=(max_len_text,))\n",
        "# Create an embedding layer to be trained, input_dim = number of volcabularies/tokens\n",
        "# output_dim is the dim of the encoded vector\n",
        "enc_emb = Embedding(input_dim = x_voc_size, output_dim = latent_dim, trainable = True)(encoder_input)\n",
        "\n",
        "# LSTM Layer 1\n",
        "# return_sequence: returns hidden state and cell state of every timestamp\n",
        "# return_state: produce hidden state and cell state of the last timestamp only\n",
        "encoder_lstm1 = LSTM(latent_dim, return_sequences=True, return_state=True,\n",
        "                     activation='tanh', \n",
        "                     dropout = params['dropout'], recurrent_dropout = params['rec_dropout'])\n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
        "#encoder_output1 = BatchNormalization(axis = 1)(encoder_output1)\n",
        "\n",
        "# LSTM Layer 2\n",
        "encoder_lstm2 = LSTM(latent_dim, return_sequences=True, return_state=True, \n",
        "                     dropout = params['dropout'], recurrent_dropout = params['rec_dropout'])\n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
        "\n",
        "# LSTM Layer 3\n",
        "encoder_lstm3 = LSTM(latent_dim, return_sequences=True, return_state=True, \n",
        "                     dropout = params['dropout'], recurrent_dropout = params['rec_dropout'])\n",
        "encoder_outputs, state_h, state_c = encoder_lstm3(encoder_output2)\n",
        "\n",
        "\n",
        "# Decoder ----------\n",
        "\n",
        "decoder_input = Input(shape=(None,))\n",
        "dec_emb_layer = Embedding(y_voc_size, latent_dim, trainable = True)\n",
        "dec_emb = dec_emb_layer(decoder_input)\n",
        "\n",
        "# LSTM Layer 1\n",
        "decoder_lstm1 = LSTM(latent_dim, return_sequences=True, return_state=True,\n",
        "                    dropout = params['dropout'], recurrent_dropout = params['rec_dropout'])\n",
        "decoder_output1, decoder_fwd_state1, decoder_back_state1 = decoder_lstm1(dec_emb,\n",
        "                                                                      initial_state=[state_h, state_c])\n",
        "\n",
        "# LSTM Layer 2\n",
        "decoder_lstm2 = LSTM(latent_dim, return_sequences=True, return_state=True,\n",
        "                    dropout = params['dropout'], recurrent_dropout = params['rec_dropout'])\n",
        "decoder_output2, decoder_fwd_state2, decoder_back_state2 = decoder_lstm2(decoder_output1)\n",
        "\n",
        "# LSTM Layer 3\n",
        "decoder_lstm3 = LSTM(latent_dim, return_sequences=True, return_state=True,\n",
        "                    dropout = params['dropout'], recurrent_dropout = params['rec_dropout'])\n",
        "decoder_outputs, decoder_fwd_state, decoder_back_state = decoder_lstm3(decoder_output2)\n",
        "\n",
        "\n",
        "# Attention Layer\n",
        "attn_layer = AttentionLayer(name='attention_layer')\n",
        "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n",
        "\n",
        "# Concat attention output and decoder LSTM output horizontally\n",
        "decoder_concat_input = Concatenate(axis = -1, name = 'concat_layer')([decoder_outputs, attn_out])\n",
        "\n",
        "# Dense layer\n",
        "decoder_dense = TimeDistributed(Dense(y_voc_size, activation = 'softmax'))\n",
        "decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "\n",
        "\n",
        "# Model -----------\n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_input, decoder_input], decoder_outputs)\n",
        "print(\"\")\n",
        "print(\"------------------ Training Model -------------------\")\n",
        "print(\"\")\n",
        "model.summary()\n",
        "if weights_to_load != '':\n",
        "  model.load_weights(full_path+weights_to_load)\n",
        "\n",
        "\n",
        "# Training -----------\n",
        "\n",
        "start_ts = dt.datetime.now()-dt.timedelta(hours=5)\n",
        "print(\"Training starts at {}\".format(start_ts))\n",
        "\n",
        "model.compile(optimizer=params['optimizer'], loss=params['loss_fun'])\n",
        "es = EarlyStopping(monitor='val_loss', mode = 'min', verbose=1, patience = params['patience'], restore_best_weights=True)\n",
        "history=model.fit([Xtrain, ytrain[:,:-1]], \n",
        "                  ytrain.reshape(ytrain.shape[0], ytrain.shape[1], 1)[:,1:],\n",
        "                  epochs=params['n_epoch'], callbacks=[es], batch_size = params['batch_size'], \n",
        "                  validation_data=([Xval, yval[:,:-1]], yval.reshape(yval.shape[0],yval.shape[1], 1)[:,1:]))\n",
        "\n",
        "end_ts = dt.datetime.now()-dt.timedelta(hours=5)\n",
        "print(\"Training ends at {}\".format(end_ts))\n",
        "print(\"Training Time: {}\".format(end_ts - start_ts))\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "# Save results -----------\n",
        "\n",
        "# Save weights\n",
        "model.save_weights(full_path+'LSTM_weights_'+ts_filename+'.h5')\n",
        "print(\"Weights saved.\")\n",
        "print(\"\")\n",
        "\n",
        "# Save CV results\n",
        "pd.DataFrame(history.history).to_csv(full_path+'LSTM_history_'+str(ts_filename)+'.csv', index = False)\n",
        "\n",
        "# Save image\n",
        "loss_plot = plt.figure()\n",
        "train_loss, val_loss = history.history['loss'], history.history['val_loss']\n",
        "plt.plot(history.epoch, train_loss, color = \"black\")\n",
        "plt.plot(history.epoch, val_loss, color = \"blue\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.legend(['Train', 'Validation'])\n",
        "#plt.show()\n",
        "loss_plot.savefig(full_path+'LSTM_loss_'+str(ts_filename)+'.jpg')\n",
        "plt.close(loss_plot)\n",
        "\n",
        "print(\"CV results saved.\")\n",
        "print(\"\")\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "# --------------------- LSTM Inference Model -------------------\n",
        "\n",
        "print(\"------------------ Inference Model -------------------\")\n",
        "\n",
        "reverse_target_word_index=y_tokenizer.index_word \n",
        "reverse_source_word_index=x_tokenizer.index_word \n",
        "target_word_index=y_tokenizer.word_index\n",
        "\n",
        "# Encoder Inference\n",
        "encoder_model = Model(inputs=encoder_input,\n",
        "                      outputs = [encoder_outputs, state_h, state_c])\n",
        "\n",
        "# Decoder inference\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_hidden_state_input = Input(shape=(max_len_text, latent_dim)) # output of encoder\n",
        "\n",
        "# Get the embeddings of the decoder sequence\n",
        "dec_emb2 = dec_emb_layer(decoder_input)\n",
        "\n",
        "# Predict the next word\n",
        "# set the initial state to the previous state\n",
        "decoder_outputs_inf1, state_h_inf1, state_c_inf1 = decoder_lstm1(dec_emb2,\n",
        "                                                   initial_state = [decoder_state_input_h,\n",
        "                                                                    decoder_state_input_c])\n",
        "decoder_outputs_inf2, state_h_inf2, state_c_inf2 = decoder_lstm2(decoder_outputs_inf1)\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm3(decoder_outputs_inf2)\n",
        "\n",
        "# Attention Inference\n",
        "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
        "\n",
        "# Concatenate attention output and decoder output\n",
        "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
        "\n",
        "# Softmax layer to output probability distribution\n",
        "decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model(inputs = [decoder_input]+[decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c],\n",
        "                      outputs = [decoder_outputs2] + [state_h2, state_c2])\n",
        "\n",
        "print(\"\")\n",
        "decoder_model.summary()\n",
        "print(\"\")\n",
        "\n",
        "\n",
        "# ------------------- Inference Helper Function -----------------\n",
        "\n",
        "# Functions to output the summary\n",
        "# by predicting the next word given the previous one\n",
        "def decode_sequence(input_seq, debug = False, threshold = 0.8):\n",
        "  # Encode the input as state vectors\n",
        "  e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
        "  # Placeholder of the target sequence\n",
        "  target_seq = np.zeros((1,1))\n",
        "  # Choose \"_START_\" as the first word\n",
        "  target_seq[0,0] = target_word_index['start']\n",
        "  stop_condition = False\n",
        "  decoded_sentence = ''\n",
        "  # Decode summary\n",
        "  while stop_condition == False:\n",
        "    # Get probabilities, hidden state, and memory cell for the next word\n",
        "    output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
        "    # Sample the best token\n",
        "    sampled_token_index = np.array(output_tokens[0,-1,:].argsort()[::-1][:2])\n",
        "\n",
        "    if sampled_token_index[0] == target_seq:\n",
        "      sampled_token_index = sampled_token_index[1]\n",
        "    else:\n",
        "      sampled_token_index = sampled_token_index[0]\n",
        "\n",
        "    sampled_token_prob = np.max(output_tokens[0,-1,:])\n",
        "    sampled_token = reverse_target_word_index[sampled_token_index]\n",
        "\n",
        "    if debug == True:\n",
        "      top_3 = np.array(output_tokens[0,-1,:].argsort()[::-1][:3])\n",
        "      top_3_word = [reverse_target_word_index[n] for n in top_3]\n",
        "      top_3_prob = sorted(output_tokens[0,-1,:])[-3:][::-1]\n",
        "      print(top_3_prob)\n",
        "      #print(output_tokens[0,-1,:])\n",
        "      print(top_3)\n",
        "      print(top_3_word)\n",
        "    # Stop?\n",
        "    if (sampled_token == 'end' or len(decoded_sentence.split())>=(max_len_summary-1)):\n",
        "      stop_condition = True\n",
        "    elif (sampled_token_prob < threshold) & (decoded_sentence != ''):\n",
        "      stop_condition = True\n",
        "    else:\n",
        "      decoded_sentence = decoded_sentence + sampled_token + ' '\n",
        "    # Update the target sequence\n",
        "    target_seq[0,0] = sampled_token_index\n",
        "    # Update inital states\n",
        "    e_h, e_c = h, c\n",
        "  return decoded_sentence[:-1]\n",
        "\n",
        "\n",
        "# Functions to convert index sequence to text\n",
        "def seq2summary(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "      if((i!=0 and i!=target_word_index['start']) and i!=target_word_index['end']):\n",
        "        newString=newString+reverse_target_word_index[i]+' '\n",
        "    return newString\n",
        "\n",
        "def seq2text(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "      if(i!=0):\n",
        "        newString=newString+reverse_source_word_index[i]+' '\n",
        "    return newString\n",
        "\n",
        "\n",
        "# ---------------------- Prediction -----------------------\n",
        "\n",
        "print(\"\")\n",
        "print(\"------------------ Prediction -------------------\")\n",
        "print(\"\")\n",
        "\n",
        "pred_start_ts = dt.datetime.now()-dt.timedelta(hours=5)\n",
        "print(\"Prediction starts at {}\".format(pred_start_ts))\n",
        "\n",
        "pred = []\n",
        "for i in range(len(ytest)):\n",
        "  pred += [decode_sequence(Xtest[i].reshape(1, max_len_text), threshold = 0.07)]\n",
        "  if i % 1000 == 0:\n",
        "    print(\"Round {} Completed, {}\".format(i, dt.datetime.now()))\n",
        "\n",
        "pred_end_ts = dt.datetime.now()-dt.timedelta(hours=5)\n",
        "print(\"Prediction ends at {}\".format(pred_end_ts))\n",
        "print(\"Prediction takes {}.\".format(pred_end_ts - pred_start_ts))\n",
        "print(\"\")\n",
        "\n",
        "pred_output = pd.DataFrame({'id': test_id,'y_pred': pred, \n",
        "                            'y_true': [' '.join(x.split()[1:-1]) for x in y_true]})\n",
        "\n",
        "pred_output.to_csv(full_path+'pred_'+str(ts_filename)+'.csv', index = False)\n",
        "\n",
        "\n",
        "# --------------------- Evaluation -------------------------\n",
        "\n",
        "import gensim.downloader as api\n",
        "word2vec = api.load(\"word2vec-google-news-300\")\n",
        "nltk.download('wordnet')\n",
        "\n",
        "eval_start = dt.datetime.now()-dt.timedelta(hours=5)\n",
        "print(\"Evaluation starts at {}\".format(eval_start))\n",
        "\n",
        "\n",
        "def sentence_bleu_n(x, weights):\n",
        "  return sentence_bleu(references = [str(x[1]).split()], \n",
        "                       hypothesis = str(x[0]).split(),\n",
        "                       weights = weights)\n",
        "  \n",
        "# bleu1\n",
        "pred_output['bleu1'] = pred_output[['y_pred', 'y_true']].apply(lambda x: sentence_bleu_n(x, weights = [1,0,0,0]), axis=1)\n",
        "\n",
        "# bleu2\n",
        "pred_output['bleu2'] = pred_output[['y_pred', 'y_true']].apply(lambda x: sentence_bleu_n(x, weights = [.5,.5,0,0]), axis=1)\n",
        "\n",
        "# bleu3\n",
        "pred_output['bleu3'] = pred_output[['y_pred', 'y_true']].apply(lambda x: sentence_bleu_n(x, weights = [.33,.33,.33,0]), axis=1)\n",
        "\n",
        "# bleu4\n",
        "pred_output['bleu4'] = pred_output[['y_pred', 'y_true']].apply(lambda x: sentence_bleu_n(x, weights = [.25,.25,.25,.25]), axis=1)\n",
        "\n",
        "# WMDistance\n",
        "get_wmd = lambda x: word2vec.wmdistance(str(x[1]), str(x[0]))\n",
        "pred_output['wmd'] = pred_output[['y_pred', 'y_true']].apply(get_wmd, axis=1)\n",
        "\n",
        "pred_output.to_csv(full_path+'pred_'+str(ts_filename)+'.csv', index = False)\n",
        "\n",
        "eval_end = dt.datetime.now()-dt.timedelta(hours=5)\n",
        "print(\"Evaluation end at {}\".format(eval_end))\n",
        "print(\"Evaluation takes {}\".format(eval_end - eval_start))\n",
        "print(\"\")\n",
        "\n",
        "# Metrics Overview\n",
        "eval_overview = pred_output[['bleu1', 'bleu2', 'bleu3', 'bleu4', 'meteor', 'wmd']].describe().iloc[1:]\n",
        "print(\"Evaluation Metrics Overview\")\n",
        "print(\"\")\n",
        "print(eval_overview)\n",
        "print(\"\")\n",
        "\n",
        "eval_overview.to_csv(full_path + 'eval_' + str(ts_filename) + '.csv', index = False)\n",
        "\n",
        "# Visualize Metrics\n",
        "plot_eval = pd.DataFrame(pred_output[['bleu1', 'bleu2', 'bleu3', 'bleu4', 'meteor', 'wmd']].unstack()).reset_index().drop(columns = 'level_1')\n",
        "plot_eval.rename(columns={'level_0':'Metrics', 0: 'Score'}, inplace = True)\n",
        "\n",
        "eval_plot = plt.figure()\n",
        "sns.boxplot(x='Metrics', y ='Score', data = plot_eval)\n",
        "plt.title('Evaluation Metrics')\n",
        "eval_plot.savefig(full_path + 'evaluation_'+ts_filename+'.jpg')\n",
        "plt.close(eval_plot)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Deep Learning Group Project - Text Summarization\n",
            "Author: Xuan Lin\n",
            "\n",
            "File timestamp: 06030018\n",
            "\n",
            "Load model weights from \n",
            "Hyper-parameters: {'title': '3L50eDORecDO', 'n_epoch': 50, 'batch_size': 500, 'optimizer': 'adam', 'loss_fun': 'sparse_categorical_crossentropy', 'max_len_text': 80, 'max_len_summary': 8, 'patience': 3, 'dropout': 0.1, 'rec_dropout': 0.1}\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Packages Loaded.\n",
            "\n",
            "Train X & y:  (284120,) (284120,)\n",
            "Validation X & y:  (31569,) (31569,)\n",
            "Test X & y:  (16000,) (16000,)\n",
            "\n",
            "Review tokenization and padding done.\n",
            "Summary tokenization and padding done.\n",
            "\n",
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "\n",
            "------------------ Training Model -------------------\n",
            "\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 80)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 80, 500)      51082500    input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 80, 500), (N 2002000     embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 80, 500), (N 2002000     lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 500)    13940500    input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, 80, 500), (N 2002000     lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, None, 500),  2002000     embedding_1[0][0]                \n",
            "                                                                 lstm_2[0][1]                     \n",
            "                                                                 lstm_2[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_4 (LSTM)                   [(None, None, 500),  2002000     lstm_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "lstm_5 (LSTM)                   [(None, None, 500),  2002000     lstm_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "attention_layer (AttentionLayer ((None, None, 500),  500500      lstm_2[0][0]                     \n",
            "                                                                 lstm_5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concat_layer (Concatenate)      (None, None, 1000)   0           lstm_5[0][0]                     \n",
            "                                                                 attention_layer[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, None, 27881)  27908881    concat_layer[0][0]               \n",
            "==================================================================================================\n",
            "Total params: 105,444,381\n",
            "Trainable params: 105,444,381\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Training starts at 2020-06-03 00:19:22.852610\n",
            "Epoch 1/50\n",
            "569/569 [==============================] - 1237s 2s/step - loss: 3.7813 - val_loss: 3.2017\n",
            "Epoch 2/50\n",
            "569/569 [==============================] - 1278s 2s/step - loss: 3.0771 - val_loss: 2.8175\n",
            "Epoch 3/50\n",
            "569/569 [==============================] - 1275s 2s/step - loss: 2.7247 - val_loss: 2.6114\n",
            "Epoch 4/50\n",
            "569/569 [==============================] - 1273s 2s/step - loss: 2.4808 - val_loss: 2.5090\n",
            "Epoch 5/50\n",
            "569/569 [==============================] - 1273s 2s/step - loss: 2.2835 - val_loss: 2.4584\n",
            "Epoch 6/50\n",
            "569/569 [==============================] - 1259s 2s/step - loss: 2.1134 - val_loss: 2.4442\n",
            "Epoch 7/50\n",
            "569/569 [==============================] - 1240s 2s/step - loss: 1.9626 - val_loss: 2.4447\n",
            "Epoch 8/50\n",
            "130/569 [=====>........................] - ETA: 15:18 - loss: 1.7852"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}